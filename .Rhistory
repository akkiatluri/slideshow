train<-H4A[inTrain,]
test<-H4A[-inTrain,]
model_lda<-lda(PopSex ~ ZYB+NOL+FOL+STB+GLS+BBH+AUB+DKB+EKB+ZOR,data = train)
pred<-predict(model_lda,newdata = test)
lda_accuracies[i]<-confusionMatrix(test$PopSex,pred$class)$overall[1]
}
mean(lda_accuracies)
plot(density(lda_accuracies))
summary(Accuracies)
summary(lda_accuracies)
lda_accuracies<-numeric()
for (i in 1:100){
inTrain<-createDataPartition(H4A$PopSex, p = .10, list = FALSE)
testing<-H4A[inTrain,]
training<-H4A[-inTrain,]
lda_model<-lda(PopSex ~ ., data = training,cv=FALSE,priors = c(1/4,1/4,1/4,1/4))
pred_lda<-predict(lda_model,newdata = testing)
lda_accuracies[i]<-confusionMatrix(testing$PopSex,(pred_lda$class))$overall[1]
}
mean(lda_accuracies)
plot(density(lda_accuracies))
summary(lda_accuracies)
model_tree rpart(PopSex ~ ZYB+NOL+FOL+STB+GLS+BBH+AUB+DKB+EKB+ZOR,data = train,parms = list(split = "gini",prior = c(1/4,1/4,1/4,1/4)))
model_tree<-rpart(PopSex ~ ZYB+NOL+FOL+STB+GLS+BBH+AUB+DKB+EKB+ZOR,data = train,parms = list(split = "gini",prior = c(1/4,1/4,1/4,1/4)))
lda_accuracies<-numeric()
for(i in 1:1000){
index0<-which(H4A$PopSex == "BERGF")
index1<-which(H4A$PopSex == "BERGM")
index2<-which(H4A$PopSex == "NORSEF")
index3<-which(H4A$PopSex == "NORSEM")
sam1<-sample(index0,round(length(index0)*.8,0), replace = FALSE)
sam2<-sample(index1,round(length(index1)*.8,0), replace = FALSE)
sam3<-sample(index2,round(length(index2)*.8,0), replace = FALSE)
sam4<-sample(index3,round(length(index3)*.8,0), replace = FALSE)
inTrain<-c(sam1,sam2,sam3,sam4)
train<-H4A[inTrain,]
test<-H4A[-inTrain,]
model_tree<-rpart(PopSex ~ ZYB+NOL+FOL+STB+GLS+BBH+AUB+DKB+EKB+ZOR,data = train,parms = list(split = "gini",prior = c(1/4,1/4,1/4,1/4)))
pred_tree<-predict(model_tree, newdata = test)
)
model_tree<-rpart(PopSex ~ ZYB+NOL+FOL+STB+GLS+BBH+AUB+DKB+EKB+ZOR,data = train,parms = list(split = "gini",prior = c(1/4,1/4,1/4,1/4)))
pred_tree<-predict(model_tree, newdata = test)
tree_accuracies<-confusionMatrix(test$PopSex,pred_tree$class)$overall[1]
tree_accuracies<-numeric()
for(i in 1:1000){
index0<-which(H4A$PopSex == "BERGF")
index1<-which(H4A$PopSex == "BERGM")
index2<-which(H4A$PopSex == "NORSEF")
index3<-which(H4A$PopSex == "NORSEM")
sam1<-sample(index0,round(length(index0)*.8,0), replace = FALSE)
sam2<-sample(index1,round(length(index1)*.8,0), replace = FALSE)
sam3<-sample(index2,round(length(index2)*.8,0), replace = FALSE)
sam4<-sample(index3,round(length(index3)*.8,0), replace = FALSE)
inTrain<-c(sam1,sam2,sam3,sam4)
train<-H4A[inTrain,]
test<-H4A[-inTrain,]
model_tree<-rpart(PopSex ~ ZYB+NOL+FOL+STB+GLS+BBH+AUB+DKB+EKB+ZOR,data = train,parms = list(split = "gini",prior = c(1/4,1/4,1/4,1/4)))
pred_tree<-predict(model_tree, newdata = test)
tree_accuracies<-confusionMatrix(test$PopSex,pred_tree$class)$overall[1]
)
tree_accuracies<-numeric()
for(i in 1:1000){
index0<-which(H4A$PopSex == "BERGF")
index1<-which(H4A$PopSex == "BERGM")
index2<-which(H4A$PopSex == "NORSEF")
index3<-which(H4A$PopSex == "NORSEM")
sam1<-sample(index0,round(length(index0)*.8,0), replace = FALSE)
sam2<-sample(index1,round(length(index1)*.8,0), replace = FALSE)
sam3<-sample(index2,round(length(index2)*.8,0), replace = FALSE)
sam4<-sample(index3,round(length(index3)*.8,0), replace = FALSE)
inTrain<-c(sam1,sam2,sam3,sam4)
train<-H4A[inTrain,]
test<-H4A[-inTrain,]
model_tree<-rpart(PopSex ~ ZYB+NOL+FOL+STB+GLS+BBH+AUB+DKB+EKB+ZOR,data = train,parms = list(split = "gini",prior = c(1/4,1/4,1/4,1/4)))
pred_tree<-predict(model_tree, newdata = test)
tree_accuracies<-confusionMatrix(test$PopSex,pred_tree$class)$overall[1]
}
tree_accuracies<-confusionMatrix(test$PopSex,pred_tree)$overall[1]
tree_accuracies<-confusionMatrix([test$PopSex,"PopSex"],predict(model_tree,newdata = test, type = "class"))$overall[1]
tree_accuracies<-confusionMatrix(test$PopSex,predict(model_tree,newdata = test, type = "class"))$overall[1]
tree_accuracies[1]<-confusionMatrix(test$PopSex,predict(model_tree,newdata = test, type = "class"))$overall[1]
tree_accuracies<-numeric()
for(i in 1:1000){
index0<-which(H4A$PopSex == "BERGF")
index1<-which(H4A$PopSex == "BERGM")
index2<-which(H4A$PopSex == "NORSEF")
index3<-which(H4A$PopSex == "NORSEM")
sam1<-sample(index0,round(length(index0)*.8,0), replace = FALSE)
sam2<-sample(index1,round(length(index1)*.8,0), replace = FALSE)
sam3<-sample(index2,round(length(index2)*.8,0), replace = FALSE)
sam4<-sample(index3,round(length(index3)*.8,0), replace = FALSE)
inTrain<-c(sam1,sam2,sam3,sam4)
train<-H4A[inTrain,]
test<-H4A[-inTrain,]
model_tree<-rpart(PopSex ~ ZYB+NOL+FOL+STB+GLS+BBH+AUB+DKB+EKB+ZOR,data = train,parms = list(split = "gini",prior = c(1/4,1/4,1/4,1/4)))
tree_accuracies[1]<-confusionMatrix(test$PopSex,predict(model_tree,newdata = test, type = "class"))$overall[1]
}
summary(tree_accuracies)
summary(tree_accuracies)
tree_accuracies<-numeric()
for(i in 1:1000){
index0<-which(H4A$PopSex == "BERGF")
index1<-which(H4A$PopSex == "BERGM")
index2<-which(H4A$PopSex == "NORSEF")
index3<-which(H4A$PopSex == "NORSEM")
sam1<-sample(index0,round(length(index0)*.8,0), replace = FALSE)
sam2<-sample(index1,round(length(index1)*.8,0), replace = FALSE)
sam3<-sample(index2,round(length(index2)*.8,0), replace = FALSE)
sam4<-sample(index3,round(length(index3)*.8,0), replace = FALSE)
inTrain<-c(sam1,sam2,sam3,sam4)
train<-H4A[inTrain,]
test<-H4A[-inTrain,]
model_tree<-rpart(PopSex ~ ZYB+NOL+FOL+STB+GLS+BBH+AUB+DKB+EKB+ZOR,data = train,parms = list(split = "gini",prior = c(1/4,1/4,1/4,1/4)))
tree_accuracies[i]<-confusionMatrix(test$PopSex,predict(model_tree,newdata = test, type = "class"))$overall[1]
}
summary(tree_accuracies)
plot(density(tree_accuracies))
Sys.setenv(PATH=paste(Sys.getenv("PATH"),"C:/Program Files/MiKTeX 2.9/miktex/bin/x64/",sep=";"))
Sys.setenv(PATH=paste(Sys.getenv("PATH"),"C:/Program Files/MiKTeX 2.9/miktex/bin/x64/",sep=";"))
Sys.getenv("PATH")
dt <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/sahdd.csv');
require(caret) #select tuning parameters
require(e1071) #SVM
set.seed(7); #replicable across our PCs? no.
# South African heart disease data
sahdd <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/sahdd.csv', as.is = T);
# sahdd$chd <- as.factor(sahdd$chd)
# exclude row number (first column)
# scale all continuous predictors, leave out famhist
SS <- cbind(sahdd["chd"], scale(sahdd[,c(-1,-6,-11)]) )
# leave as is, include family history
SSU <- sahdd
SS[,"chd"] <- as.factor(SS[,"chd"])
SSU[,"chd"] <- as.factor(SSU[,"chd"])
SSU[,"famhist"] <- as.factor(SSU[,"famhist"])
# or SS$chd <- as.factor(SS$chd)
require(caret) #select tuning parameters
require(e1071) #SVM
set.seed(7); #replicable across our PCs? no.
# South African heart disease data
sahdd <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/sahdd.csv', as.is = T);
# sahdd$chd <- as.factor(sahdd$chd)
# exclude row number (first column)
# scale all continuous predictors, leave out famhist
SS <- cbind(sahdd["chd"], scale(sahdd[,c(-1,-6,-11)]) )
# leave as is, include family history
SSU <- sahdd
SS[,"chd"] <- as.factor(SS[,"chd"])
SSU[,"chd"] <- as.factor(SSU[,"chd"])
SSU[,"famhist"] <- as.factor(SSU[,"famhist"])
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 10)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
Accuracies<-numeric()
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 10)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
sink()
summary(Accuracies)
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
Accuracies<-numeric()
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 5)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 20, repeats = 5)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
summary(Accuracies)
Accuracies<-numeric()
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 2, repeats = 1)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
summary(Accuracies)
Accuracies<-numeric()
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 2, repeats = 10)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
Accuracies<-numeric()
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 1)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
Accuracies<-numeric()
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 100, maxit = 20, repeats = 5)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
Howells$PopSex <- as.factor(Howells$PopSex)
# many columns are Nas
Howells <- na.omit(Howells[,c(1,2,4,5:61,63,67:80)])
attach(Howells);
H4A <- Howells[which(Pop == 'NORSE' | Pop == 'BERG'),];
# Peruvian males
HP <- Howells[which(PopSex == 'PERUM'),];
# ALL males
HowMs <- Howells[which(Sex == 'M'),];
library(MASS);
# column one name is now HID
HP11 <- HP[,c("HID","PopSex","GOL","NOL","BNL","BBH","XCB","ZYB","WNB","AUB","ZMB","NAR","DKB")]
Accuracies<-numeric()
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-10, maxit = 1000, repeats = 100)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
Accuracies<-numeric()
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 100, repeats = 50)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
Accuracies<-numeric()
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-1, maxit = 10, repeats = 1)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
Accuracies<-numeric()
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-1, maxit = 1000, repeats = 1)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
Accuracies<-numeric()
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-10, maxit = 10, repeats = 1)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
Accuracies<-numeric()
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 100, maxit = 10, repeats = 1)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
Accuracies <- c(0.00)
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(H4AS$PopSex, p = .80, list = FALSE)
H4AS.navg <- avNNet(PopSex ~ ., data = H4AS, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 1000, repeats =100
10)
Accuracies[i] <- confusionMatrix(H4AS$PopSex[-inTrain], predict(H4AS.navg, H4AS[-inTrain,], type =
"class"))$overall["Accuracy"]
#print(paste('Ran the process ', i, 'times.'))
}
summary(Accuracies)
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
attach(Howells)
H4A <- Howells[which(Pop == 'NORSE' | Pop == 'BERG'),];
# many columns are Nas
H4A <- na.omit(H4A[,c(5:61,63,67:80)]);
H4A$PopSex <- as.factor(H4A$PopSex);
# standardize vars ALWAYS with NN
H4AS <- cbind(H4A["PopSex"], scale(H4A[2:72]) )
Accuracies <- c(0.00)
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(H4AS$PopSex, p = .80, list = FALSE)
H4AS.navg <- avNNet(PopSex ~ ., data = H4AS, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 1000, repeats =100
10)
Accuracies[i] <- confusionMatrix(H4AS$PopSex[-inTrain], predict(H4AS.navg, H4AS[-inTrain,], type =
"class"))$overall["Accuracy"]
#print(paste('Ran the process ', i, 'times.'))
}
summary(Accuracies)
Accuracies <- c(0.00)
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(H4AS$PopSex, p = .80, list = FALSE)
H4AS.navg <- avNNet(PopSex ~ ., data = H4AS, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 1000, repeats =100)
Accuracies[i] <- confusionMatrix(H4AS$PopSex[-inTrain], predict(H4AS.navg, H4AS[-inTrain,], type =
"class"))$overall["Accuracy"]
#print(paste('Ran the process ', i, 'times.'))
}
summary(Accuracies)
Accuracies <- c(0.00)
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(H4AS$PopSex, p = .80, list = FALSE)
H4AS.navg <- avNNet(PopSex ~ ., data = H4AS, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 100, repeats =10)
Accuracies[i] <- confusionMatrix(H4AS$PopSex[-inTrain], predict(H4AS.navg, H4AS[-inTrain,], type =
"class"))$overall["Accuracy"]
#print(paste('Ran the process ', i, 'times.'))
}
summary(Accuracies)
Accuracies <- c(0.00)
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(H4AS$PopSex, p = .80, list = FALSE)
H4AS.navg <- avNNet(PopSex ~ ., data = H4AS, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 10, repeats =1)
Accuracies[i] <- confusionMatrix(H4AS$PopSex[-inTrain], predict(H4AS.navg, H4AS[-inTrain,], type =
"class"))$overall["Accuracy"]
#print(paste('Ran the process ', i, 'times.'))
}
summary(Accuracies)
Accuracies <- c(0.00)
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(H4AS$PopSex, p = .80, list = FALSE)
H4AS.navg <- avNNet(PopSex ~ ., data = H4AS, subset = inTrain, size = 2, rang = 0.5, decay = 5e-1, maxit = 10, repeats =1)
Accuracies[i] <- confusionMatrix(H4AS$PopSex[-inTrain], predict(H4AS.navg, H4AS[-inTrain,], type =
"class"))$overall["Accuracy"]
#print(paste('Ran the process ', i, 'times.'))
}
summary(Accuracies)
Accuracies <- c(0.00)
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(H4AS$PopSex, p = .80, list = FALSE)
H4AS.navg <- avNNet(PopSex ~ ., data = H4AS, subset = inTrain, size = 2, rang = 0.5, decay = 5e-1, maxit = 1000, repeats =100)
Accuracies[i] <- confusionMatrix(H4AS$PopSex[-inTrain], predict(H4AS.navg, H4AS[-inTrain,], type =
"class"))$overall["Accuracy"]
#print(paste('Ran the process ', i, 'times.'))
}
summary(Accuracies)
Accuracies <- c(0.00)
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(H4AS$PopSex, p = .80, list = FALSE)
H4AS.navg <- avNNet(PopSex ~ ., data = H4AS, subset = inTrain, size = 2, rang = 0.5, decay = 5e-1, maxit = 1000, repeats =1)
Accuracies[i] <- confusionMatrix(H4AS$PopSex[-inTrain], predict(H4AS.navg, H4AS[-inTrain,], type =
"class"))$overall["Accuracy"]
#print(paste('Ran the process ', i, 'times.'))
}
summary(Accuracies)
Accuracies <- c(0.00)
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(H4AS$PopSex, p = .80, list = FALSE)
H4AS.navg <- avNNet(PopSex ~ ., data = H4AS, subset = inTrain, size = 2, rang = 0.5, decay = 5e-10, maxit = 1000, repeats =1)
Accuracies[i] <- confusionMatrix(H4AS$PopSex[-inTrain], predict(H4AS.navg, H4AS[-inTrain,], type =
"class"))$overall["Accuracy"]
#print(paste('Ran the process ', i, 'times.'))
}
summary(Accuracies)
Accuracies<-numeric()
for (i in seq(50))
{
# sample
inTrain <- createDataPartition(SSU$chd, p = .80, list = FALSE)
SS.navg <- avNNet(SS[c(2:9)], SS$chd, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 100, repeats = 1)
Accuracies[i] <- confusionMatrix(SS$chd[-inTrain], predict(SS.navg, SS[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
library(gutenbergr)
dracula<-gutenberg_download(345)
library(tidytext)
dracula
library(ggplot2)
dracula_words<- dracula%>%
unnest_tokens(word,text)
library(dplyr)
dracula_words<- dracula%>%
unnest_tokens(word,text)
dracula
dracula_words
bing<-get_sentiments('bing')
bing
dracula_words<-inner_join(dracula_words,bing)
dracula_words
dracula_words$gutenberg_id<-NULL
dracula_words%>%
filter(sentiment=='positive')
dracula_pos<-dracula_words%>%
filter(sentiment=='positive')
dracula_pos<-dracula_words%>%
filter(sentiment=='positive')
dracula_pos<-dracula_words%>%
filter(sentiment=='positive')%>%
group_by(word)%>%
summarize(count=n())
dracula_pos
dracula_pos<-dracula_words%>%
filter(sentiment=='positive')%>%
group_by(word)%>%
summarize(count=n())%>%
arrange(count)
dracula_pos
dracula_pos<-dracula_words%>%
filter(sentiment=='positive')%>%
group_by(word)%>%
summarize(count=n())%>%
arrange(count)%>%
filter(count>=66)
dracula_pos$word<-as.factor(dracula_pos$word)
class(dracula_pos$word)
ggplot()+
geom_bar(data = dracula_pos,aes(x=word,y=count,stat = 'identity'))
ggplot()+
geom_bar(data = dracula_pos,aes(x=word,y=count),stat = 'identity')
data(prostate)
require(rpart) #classification and regression trees (CART)
require(partykit) #treeplots
require(MASS) #breast and pima indian data
require(ElemStatLearn) #prostate data
require(randomForest) #random forests
require(gbm) #gradient boosting
require(caret) #tune hyper-parameters
require(caret) #tune hyper-parameters
require(gbm) #gradient boosting
install.packages("gbm")
require(randomForest) #random forests
require(ElemStatLearn) #prostate data
install.packages("‘ElemStatLearn")
require(MASS) #breast and pima indian data
require(partykit) #treeplots
install.packages("‘partykit")
prostate$gleason = ifelse(prostate$gleason == 6, 0, 1)
pros.train = subset(prostate, train==TRUE)[,1:9]
pros.test = subset(prostate, train==FALSE)[,1:9]
# very simple call, many defaults
tree.pros = rpart(lpsa~., data=pros.train)
print(tree.pros$cptable)
data(prostate)
require(ElemStatLearn) #prostate data
install.packages("‘ElemStatLearn")
setwd("C:/Users/Akki/Desktop/slideshow-master")
